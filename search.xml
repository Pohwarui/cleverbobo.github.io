<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>深度理解逻辑回归</title>
    <url>/2020/08/27/log-regression/</url>
    <content><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>&emsp;&emsp;所谓机器学习，听起来很高深莫测，实际上并不复杂，简单来说就是数据驱动的算法。而数据驱动就是指在给定的数据情况下，我们需要找到一种合适的算法对这些数据进行操作，从而实现我们预期的目标，具体我们需要做的任务就是找到合适的模型来描述输入到输出之间的映射关系，然后在使用优化的方法不断对模型中的参数进行优化，使得最后得到的结果鲁棒性最高。在上述的描述中，主要涉及两个方面：1.合适的模型 2.合适的优化算法。其实，我们在初中就接触过相关的领域，比如给定一系列坐标点（x，y），利用线性回归公式（最小二乘法）拟合曲线y=ax+b。其中，y=ax+b就是我们选择的模型，而最小二乘法就是对其参数a，b的优化算法。当然，这只是最简单的应用，实际机器学习的算法往往要比这个复杂很多，但究其本质还是一样的。<br>&emsp;&emsp;对于机器学习而言，其功能非常强大，能完成分类、回归、转录、机器翻译、异常与检测、合成和采样等等。其中，分类与回归是机器学习的最基本的两项功能。上述举的例子就是回归算法。而今天，我主要介绍一种常用的分类算法——逻辑回归。对于分类的而言，逻辑回归是一种重要的学习方式，该方法所假设的函数在经过优化后的到模型适用性非常高。另外，之所以该方法在名称中带有回归二字，是因为其算法原理与线性回归之间有很深的联系，在下面介绍算法原理的时候我会重点讲解。</p>
<a id="more"></a>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>&emsp;&emsp;分类，顾名思义，就是将输入的数据分为不同的类别，其结果是离散的，比如预测明天的天气是去晴天还是非晴天。在计算机中，我们往往使用不同的数字代表不用的类别，比如1代表晴天，0代表非晴天。<br>&emsp;&emsp;但是，如果算法仅仅只能够告诉我们分类的结果是非常不精准的，我们更期望算法能够告诉我们发生某种情况的概率，比如明天晴天的概率为80%，非晴天的概率为20%，这样的话我们的可操作性就会更强，可以人为的添加参数（优化）对算法进行矫正。仍以晴天为例，如果分类算法较为精准，我们可以用50%为阈值，如果晴天的概率大于50%，就是晴天；如果算法计算晴天概率比实际偏大，那我们可以设置60%为阈值，大于60%为晴天</p>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>&emsp;&emsp;在正式讲解逻辑回归的算法之间，首先我们需要了解一下线性回归的基本原理。我们假定输出y由输入x线性决定（这里的x,y都是向量)，其表达式为：</p>
<script type="math/tex; mode=display">f(x)=\theta^{T} x······（1）</script><p>当输入变量只有一个时，就变成了我们熟悉的y=ax+b，此时，利用最小二乘法，具体的效果大致如下：<br><img src="/2020/08/27/log-regression/1.png" alt="y=ax+b"><br>&emsp;&emsp;当多个变量的时候，就由一维向量到多维，例如二维就会得到一个平面，而非一条直线了，其共同的特点就是模型是连续的，其值域为（-∞，+∞），这是线性回归模型的一个重要的特征。</p>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>&emsp;&emsp;逻辑回归是一种回归算法，该种算法主要应用于二分类的状况，例如明天北京是否会下雪，一个人五年内是否会得心脏病等等，由线性回归延伸出来的，那么具体是如何出现的呢？下面我来具体说明，仍以晴天为例：</p>
<ol>
<li>对于明天天气如何，我们该如何预测呢？首先，对于给定输入，比如温度，云层厚度，时间，风力，这些参数在算法中表现为x1,x2,x3……，我们最容易想到的就是令预测概率p（x）为x的线性函数，这样就和线性回归一致了，其值域为（-∞，+∞），并不符合我们的要求（概率应该在0~1之间），因此我们需要对其进行改进，使其符合我们的要求；</li>
<li>如果我们需要限制值域，在机器学习中最常用的就是ln函数，因此我们做一个简单的调整，令lnp(x)为x的线性函数，也就是说p（x）=exp(ax+b)，但是该函数无论正负，均只能在一个方向上约束值域，因此还需要改进；</li>
<li>最后，对lnp作简单的调整，令其在两个方向都被约束，我们用的方法是逻辑转换，令ln(p/1-p)为x的线性函数，那么p（x）的值域就是[0,1]<br>&emsp;&emsp;因此逻辑回归的表达式为（在印刷体中我们往往都采用log代表ln）：<script type="math/tex; mode=display">\log \frac{p(x)}{1-p(x)}=\beta_{0}+x \beta··········（2）</script></li>
</ol>
<p>如（2）中所示，p（x）为事情发生的概率，令$\log \frac{p(x)}{1-p(x)}$成为x的线性函数，解得p(x)为：</p>
<script type="math/tex; mode=display">\mathrm{p}\left(x ; \beta_{0}, \beta\right)=\frac{1}{1+e^{-\left(\beta_{0}+x \beta\right)}}··········（3）</script><p>（3）式相比于（2）式更容易理解条件概率p（x），但（2）式更能凸显逻辑回归与线性回归之间的关系。<br>如图所示：<br><img src="/2020/08/27/log-regression/2.png" alt="逻辑回归"><br>该图为函数逻辑回归曲线的大致形状，基本完成了我们期望的条件：曲线较为平滑，值域位于[0,1]。</p>
<h2 id="主要特征"><a href="#主要特征" class="headerlink" title="主要特征"></a>主要特征</h2><ol>
<li>一般取$\beta_{0}+x \quad \beta=0$为分类的边界，那么如果x是一维，那么分类边界就是一个点（类似在数轴上分类）；如果是二维分类边界就是一条直线，以此类推。之所以如此设置，是因为设置β0+ x β= 0为边界后，我们可以认为当算法输出的概率p（x）≥0.5时，分类结果 Y为1,；当p&lt;0.5时，分类结果Y为0。或者说算法输入的$\beta_{0}+x \quad \beta≥0$时，分类结果为1；$\beta_{0}+x \quad \beta&lt;0$时，分类结果为0（目前只考虑二分类的情况，多种分类情况在后面有介绍）。这样我们就可以把前面计算得到的概率转换为分类的结果了，既得到了分类的条件概率，又得到了分类的结果。</li>
<li>逻辑回归计算得到的条件概率是由数据点到边界之间的距离决定的，为$\frac{\beta_{0}+x \beta}{|\beta|}$。也就是说如果距离边界远，那么为1（或者0）的概率就会越大。另外这个公式也说明了当||β||越大时，在同一数据集下，分类得到的概率会更像极端（0,1）靠近，如下图所示<br><img src="/2020/08/27/log-regression/3.png" alt="逻辑回归"><br>（备注：最后一种是用线性分类的方式进行分类的）</li>
<li>逻辑回归是跟据线性回归演化而来，提出时间早，科学家们对其研究较为透彻，运用较为熟练。此外，该算法较为简单，且适用性较强，机器学习的算法在准确度够得情况下，尽量选择较为简单的算法，避免出现过拟合</li>
</ol>
<h2 id="对于多分类的逻辑回归"><a href="#对于多分类的逻辑回归" class="headerlink" title="对于多分类的逻辑回归"></a>对于多分类的逻辑回归</h2><p>逻辑回归本质上是对二分类的事件进行的分类，但是对于多分类的情况，我们可以通过建立多个分类模型，将多分类差分成多个二分类的情况，在利用逻辑回归进行分类，如图所示：<br><img src="/2020/08/27/log-regression/4.png" alt="逻辑回归"><br>一般而言如果存在n中情况，我们会设置n个分类器，每个分类器对其中一种进行识别，其条件概率的计算公式为：</p>
<script type="math/tex; mode=display">P(Y=c \mid X=x)=\frac{e^{\beta_{0}^{(c)}+x \beta^{(c)}}}{\sum_{i=1}^{n} e^{\beta_{0}^{(j)}+x \beta^{(i)}}}</script><p>判别方法与之前类似，如果超过0.5就认为是第c类。值得一提是，当只有两种情况时，令$\beta^{0}=\beta_{0}^{(1)}-\beta_{0}^{(0)}$以及$\beta=\beta^{(1)}-\beta^{(0)}$时，多情况的逻辑分类就转换成二分类的情况了。<br><br>下面是我自己的推导过程：<br><img src="/2020/08/27/log-regression/5.png" alt="逻辑回归"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>入门</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>markdown基本语法一</title>
    <url>/2020/08/26/markdown1/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇blog主要面向初步接触markdown的小白，主要是面向windows用户，简单介绍markdown语法规则，同时方便自己在忘记时能快速查看~<br>本文参考文献：<br><a href="https://hyxxsfwy.github.io/2016/01/15/Hexo-Markdown-%E7%AE%80%E6%98%8E%E8%AF%AD%E6%B3%95%E6%89%8B%E5%86%8C">一、简明语法手册</a><br><a href="https://www.mereith.com/2018/12/08/markdown%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/">二、某知名哲学家的blog</a><br><a href="https://www.jianshu.com/p/191d1e21f7ed/">三、Markdown基本语法</a><br><a href="https://www.jianshu.com/p/599857933f6e">四、Markdown基本语法总结</a></p>
<h2 id="创建新文章"><a href="#创建新文章" class="headerlink" title="创建新文章"></a>创建新文章</h2><p>第一种：打开powershell（管理员），打开到对应位置的目录，输入以下代码<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo new <span class="string">&quot;新文章的名字&quot;</span></span><br></pre></td></tr></table></figure><br>第二种：打开blog文章所在的位置，例如<code>D:\Program Files\hexo-blog\myblog\source\_posts</code>,创建txt文件，更改拓展名为.md</p>
<p>两种方法没有什么本质区别，效率也是一样的，大家可以根据自己的习惯选择,个人推荐选择第一种~</p>
<h2 id="创建blog文件的文件头部分"><a href="#创建blog文件的文件头部分" class="headerlink" title="创建blog文件的文件头部分"></a>创建blog文件的文件头部分</h2><p>每一篇blog一般都要包含4个部分，以本篇为例：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">title: markdown基本语法一</span><br><span class="line">date: 2020-08-26 23:00:00</span><br><span class="line">tags: [markdown,入门] </span><br><span class="line">categories: markdown</span><br><span class="line">description: 本篇blog主要面向初步接触markdown的小白，主要是面向windows用户，简单介绍markdown语法规则，同时方便自己在忘记时能快速查看~</span><br></pre></td></tr></table></figure><br>分别对应本篇文章的标题，编辑时间，标签以及文章分类。另外，标签可以有多个，用英文逗号隔开；分类建议只有一个；简介只在首页列表显示，打开后不会显示，是可选项。<br><a id="more"></a></p>
<h2 id="编写文章的标题"><a href="#编写文章的标题" class="headerlink" title="编写文章的标题"></a>编写文章的标题</h2><p>markdown支持6级标题，以<code>#</code>作为关键字，识别标题，实例如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 一级标题</span></span><br><span class="line"><span class="comment">## 二级标题</span></span><br><span class="line"><span class="comment">### 三级标题</span></span><br><span class="line"><span class="comment">#### 四级标题</span></span><br><span class="line"><span class="comment">##### 五级标题</span></span><br><span class="line"><span class="comment">###### 六级标题</span></span><br></pre></td></tr></table></figure><br>效果如下：<br><img src="https://pic.mereith.com/img/show_title.png-slim" alt></p>
<p>此外还有一些特殊的标题符号，例如在标题前加入<code>-</code>, <code>*</code>,<code>+</code>，可着重标记该项，效果如下：</p>
<ul>
<li>着重强调符号</li>
</ul>
<h2 id="字体设置"><a href="#字体设置" class="headerlink" title="字体设置"></a>字体设置</h2><h3 id="粗体和斜体"><a href="#粗体和斜体" class="headerlink" title="粗体和斜体"></a>粗体和斜体</h3><p>使用<code>*</code>和<code>**</code>分别表示斜体和粗体，格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">*斜体*，**粗体**，***斜体加粗***</span><br></pre></td></tr></table></figure><br>效果展示：<em>斜体</em>,<strong>粗体</strong>,<strong><em>斜体加粗</em></strong></p>
<h2 id="字体、字号、颜色"><a href="#字体、字号、颜色" class="headerlink" title="字体、字号、颜色"></a>字体、字号、颜色</h2><p>使用关键字，可以指定字体的颜色和大小，其格式为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">指定字体类型： &lt;font face=<span class="string">&quot;黑体&quot;</span>&gt;我是黑体字&lt;/font&gt;</span><br><span class="line">指定字体大小： &lt;font size=12&gt;我是12号字&lt;/font&gt;</span><br><span class="line">指定字体颜色：&lt;font color=<span class="comment">#0099ff&gt;我是蓝色字&lt;/font&gt; #0099ff 为颜色的16进制代码</span></span><br><span class="line">指定字体颜色、字号、字体类型&lt;font color=<span class="comment">#0099ff size=12 face=&quot;黑体&quot;&gt;黑体&lt;/font&gt;</span></span><br></pre></td></tr></table></figure><br>效果如下：<br>指定字体类型： <font face="黑体">我是黑体字</font><br>指定字体大小： <font size="12">我是12号字</font><br>指定字体颜色：<font color="#0099ff">我是蓝色字</font> #0099ff 为颜色的16进制代码<br>指定字体颜色、字号、字体类型<font color="#0099ff" size="12" face="黑体">黑体</font></p>
<h3 id="换行"><a href="#换行" class="headerlink" title="换行"></a>换行</h3><p>方法1：连敲2个以上空格+enter键；<br>方法2：利用html语法，<code>&lt;br&gt;</code>。</p>
<h3 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h3><p>单独一行使用<code>***</code>或者<code>---</code>，表示该行作为分割线</p>
<hr>
<h3 id="删除线"><a href="#删除线" class="headerlink" title="删除线"></a>删除线</h3><p>删除线是指在原文本上画一条线，类似在纸上写错了划线来删除，一般用于表示过时的版本或者错误的写法较为醒目，格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">~~删除线~~</span><br></pre></td></tr></table></figure><br>效果：<del>删除线</del></p>
<h3 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h3><p>在编写blog的时候，往往会参考一些网站，我们需要把网站链接放在正文中，此时就需要用到超链接；此外还可以放一些图片链接<br>网站链接格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[链接说明](链接地址)</span><br></pre></td></tr></table></figure><br>例如：<a href="https://www.mereith.com/">某知名哲学家♂的个人主页</a></p>
<p>图片超链接格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">![图片说明](图片地址)</span><br></pre></td></tr></table></figure><br>例如：<img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=543966357,2530364137&amp;fm=26&amp;gp=0.jpg" alt="某知名老婆的图片" title="蕾姆"></p>
<h3 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h3><p>可以用<code>\</code>来表示注释，也就是说<code>\</code>后的文字不会被转义，格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">\<span class="comment">#标题格式，但不是标题</span></span><br></pre></td></tr></table></figure><br>具体效果如下：#标题格式，但不是标题</p>
<h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><p>使用<code>&gt;</code>来表示文字的引用，往往在引用参考文献中的语句时使用，其格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; Sow nothing, reap nothing</span><br></pre></td></tr></table></figure><br>此外，引用还可以嵌套使用，例如:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; 这是第一个引用</span><br><span class="line">&gt;&gt; 这是第一个引用中的引用</span><br><span class="line">&gt;&gt;&gt; 这是第一个引用中的引用的引用</span><br></pre></td></tr></table></figure><br>效果展示如下：</p>
<blockquote>
<p>这是第一个引用</p>
<blockquote>
<p>这是第一个引用中的引用</p>
<blockquote>
<p>这是第一个引用中的引用的引用<br>我就不套娃了，大家自己可以试试~</p>
<h3 id="代码块"><a href="#代码块" class="headerlink" title="代码块"></a>代码块</h3><p><strong>首先</strong>，是行内代码块，使用一对 <code>来括住文字，格式如下：
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">行内`代码`块</span><br></pre></td></tr></table></figure>
其效果如下：行内</code>代码`块</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>其次</strong>，多行代码块，使用一对```来括住文字，效果如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第一行</span><br><span class="line">第二行</span><br><span class="line">第三行</span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<p><strong>最后</strong>，支持规定语言的代码展示，以python为例，在 ``` 后加入编码格式即可<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(<span class="string">&quot;hello world&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="引用本地图片"><a href="#引用本地图片" class="headerlink" title="引用本地图片"></a>引用本地图片</h2><p>在hexo中，如果你想引用本地图片，最好先安装hexo-asset-image的插件，保证图片不会因为移动而丢失。安装完该插件之后，每次产生新的.md文件的同时，还会在生成与之同名的文件夹。将想要上传的图片传入该文件夹，按照<code>![你想输入的替代文字](xxxx/图片名.jpg)</code>引用即可，效果如下：<br><img src="/2020/08/26/markdown1/test1.jpg" alt="测试图片"></p>
]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>入门</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>markdown进阶心得（更新中）</title>
    <url>/2020/08/27/markdown2/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇blog主要是在编辑markdown时，遇到了很多问题，同时也找到了很多技巧，所以在这里mark一下</p>
<h2 id="首行缩进"><a href="#首行缩进" class="headerlink" title="首行缩进"></a>首行缩进</h2><p>英文字符空格 <code>&amp;ensp;</code><br>中文字符空格 <code>&amp;emsp;</code><br>不断行的空白格 <code>&amp;nbsp;</code><br>其中较为常用的是<code>&amp;emsp;</code>,其效果如下：<br>&emsp;&emsp;<strong>首行缩进两字符</strong><br><a id="more"></a></p>
<h2 id="文字居中"><a href="#文字居中" class="headerlink" title="文字居中"></a>文字居中</h2><p>文字居中使用<code>&lt;center&gt; ``&lt;/center&gt;</code>来括住文字，格式如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;center&gt; 文字居中&lt;&#x2F;center&gt; </span><br></pre></td></tr></table></figure><br>效果展示：</p>
<center> 文字居中</center> 

<h2 id="插入公式"><a href="#插入公式" class="headerlink" title="插入公式"></a>插入公式</h2><p>参考链接：(<a href="https://www.jianshu.com/p/7ab21c7f0674">https://www.jianshu.com/p/7ab21c7f0674</a>)<br>按照参考链接里面的教程，将冲突的配置文件更改，即可在exo中渲染MathJax数学公式<br><br>但是该公式有严格的LaTeX语法，语法规则参考<a href="https://www.jianshu.com/p/25f0139637b7">《markdown中公式编辑教程》</a>,所以较为复杂，在这里推荐一种懒人方法：使用“mathpix”插件，官方的下载地址为(<a href="https://mathpix.com/)。">https://mathpix.com/)。</a><br>使用方法浅显易懂，注册账号登录，截屏选取公式，复制结果。如果是编辑公式，我们可以在word上编写或者写在纸上，用该软件转换格式，实测非常好用~</p>
<h2 id="writage插件"><a href="#writage插件" class="headerlink" title="writage插件"></a>writage插件</h2><p>官方地址：(<a href="https://www.writage.com">https://www.writage.com</a>)<br>使用教程：(<a href="https://www.cnblogs.com/craigtaylor/p/13540170.html">https://www.cnblogs.com/craigtaylor/p/13540170.html</a>)<br>该插件可以将word文件转换为.md文件，但是经过我的实测，并不是很理想，待开发中~</p>
<center> <font size="12" color="#DC143C" face="黑体">未完待续~</font> </center>

]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>进阶</tag>
      </tags>
  </entry>
  <entry>
    <title>通俗理解奇异值分解</title>
    <url>/2020/08/27/svd-pca/</url>
    <content><![CDATA[<p>参考链接：</p>
<ol>
<li>[<a href="https://zhuanlan.zhihu.com/p/29846048">https://zhuanlan.zhihu.com/p/29846048</a>]</li>
<li>[<a href="https://www.zhihu.com/question/41120789/answer/481966094">https://www.zhihu.com/question/41120789/answer/481966094</a>]</li>
</ol>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>&emsp; &emsp;机器算法的核心就是如何妥善的处理数据，但是当我们接触到一大堆原始杂乱的陌生数据时，往往会感到手足无措，因此我们需要对原始数据压缩，从而找到影响结果变化的主要因素与次要因素，奇异值分解就为我们提供了相应的方法，便于我们从茫茫的数据中找到关键因素，当然这种方法不仅仅局限于数据压缩，还有其他强大的功能，在此就不一 一介绍了。</p>
<a id="more"></a>
<h2 id="特征值分解"><a href="#特征值分解" class="headerlink" title="特征值分解"></a>特征值分解</h2><p>&emsp; &emsp;在正式接触奇异值分解之前，我们先来回顾在线性代数中学到的特征值分解(对角化)。首先特征值分解具有一定的局限性，只能应用于方阵，并且该方阵还要满足每个特征值对应的特征向量线性无关的最大个数等于该特征值的重数才能够对角化。对于n维可对角化的矩阵A可写成以下形式：</p>
<script type="math/tex; mode=display">A=V \Lambda V^{-1}</script><p>具体表现形式为：</p>
<script type="math/tex; mode=display">A=\left[\begin{array}{}
\mid & \mid & \mid \\
\mathbf{v}_{1} & \mathbf{v}_{2} & \mathbf{v}_{3} \\
\mid & \left.\right|& \mid
\end{array}\right]\left[\begin{array}{}
\left.\mid \begin{array}{}
\lambda_{1} & 0 & 0 \\
0 & \lambda_{2} & 0 \\
0 & 0 & \lambda_{3}
\end{array}\right]\left[\left.\begin{array}{}
\mid & \mid & \mid \\
\mathbf{v}_{1} & \mathbf{v}_{2} & \mathbf{v}_{3}\\
\mid & \left.\right|& \mid
\end{array}\right]^{-1}\right.
\end{array}\right.</script><p>上式中，v1,v2,v3为矩阵A的特征向量，一般情况下我们会把特征向量标准化，即满足|| V ||=1。然而，这种特征值分解只能应用于部分方阵。然而，在我们实际的数据中，大部分都并非方阵，因此我们需要对这种方法进行拓展，从而提出了奇异值分解，来应对普遍情况。</p>
<h2 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h2><p>&emsp; &emsp;$ A A^{T} $与$ A^{T} A $这两个矩阵对于奇异值分解来说是十分重要的两种矩阵，它们具有以下特征：<br><br>&emsp; &emsp;&emsp; &emsp;1.对于任意维度的矩阵A，都存在$ A A^{T} $与$ A^{T} A $；<br><br>&emsp; &emsp;&emsp; &emsp;2.实对称矩阵，特征向量正交；<br>&emsp; &emsp;&emsp; &emsp;3.方阵；<br>&emsp; &emsp;&emsp; &emsp;4.半正定；<br>&emsp; &emsp;&emsp; &emsp;5.两者具有相同的正数特征值，该特征值得算术平方根称之为奇异值<br>&emsp; &emsp;&emsp; &emsp;6.两者的秩相等，且与矩阵A相同。<br>&emsp; &emsp;这样的话，我们令$ u_{i} $为矩阵$ A A^{T} $的特征向量，令$ v_{i} $为矩阵$ A^{T} A $的特征向量,则我们称由$ u_{i} $，$ v_{i} $构成的矩阵$U$，$V$为矩阵A的奇异向量，具体表达式如下：<br><img src="/2020/08/27/svd-pca/1.png" alt><br>也就是说：</p>
<script type="math/tex; mode=display">\left(A^{T} A\right) v_{i}=\lambda_{i} v_{i}</script><script type="math/tex; mode=display">\left(A A^{T}\right) u_{i}=\lambda_{i} u_{i}</script><p>&emsp; &emsp;根据之前$ A A^{T} $与$ A^{T} A $的性质可知，我们可以通过正交分解使得$U,V$为正交矩阵。有了上述的基础概念，下面引入核心公式，奇异值分解认为，任意维度的矩阵A都可以被分解为以下形式：</p>
<script type="math/tex; mode=display">A=U S V^{T}</script><p>其中，$U,V$就是矩阵A的奇异向量；S是对角阵，由奇异值构成。矩阵的排列顺序，一般按照奇异值从大到小排列，方便统一化管理和后续操作：<br><img src="/2020/08/27/svd-pca/2.png" alt><br>那么奇异值是如何计算的呢？</p>
<script type="math/tex; mode=display">A=U \Sigma V^{T} \Rightarrow A^{T}=V \Sigma U^{T} \Rightarrow A^{T} A=V \Sigma U^{T} U \Sigma V^{T}=V \Sigma^{2} V^{T}</script><p>所以说奇异值矩阵等于特征值矩阵的算术平方根。相比于之前的特征值分解，SVD可以应用于任意维度的矩阵，并且U,V是正交矩阵，S是对角阵，具体应用的时候会非常的方便。</p>
<h2 id="奇异值分解矩阵"><a href="#奇异值分解矩阵" class="headerlink" title="奇异值分解矩阵"></a>奇异值分解矩阵</h2><p>为了让大家有一个深刻的印象，我举一个简单的例子来具体说明SVD是如何分解矩阵的。<br>对于矩阵A：</p>
<script type="math/tex; mode=display">A=\left(\begin{array}{ccc}
3 & 2 & 2 \\
2 & 3 & -2
\end{array}\right)</script><p>计算$ A A^{T} $与$ A^{T} A $的值得：</p>
<script type="math/tex; mode=display">A A^{T}=\left(\begin{array}{cc}
17 & 8 \\
8 & 17
\end{array}\right), \quad A^{T} A=\left(\begin{array}{ccc}
13 & 12 & 2 \\
12 & 13 & -2 \\
2 & -2 & 8
\end{array}\right)</script><p>上述矩阵都是半正定的，其特征值为25、9，也就是说A的奇异值为5、2。另外，这两个矩阵之间的联系为：<br><img src="/2020/08/27/svd-pca/3.png" alt><br>因此SVD的表达式为：</p>
<script type="math/tex; mode=display">A=U S V^{T}=\left(\begin{array}{cc}
1 / \sqrt{2} & 1 / \sqrt{2} \\
1 / \sqrt{2} & -1 / \sqrt{2}
\end{array}\right)\left(\begin{array}{ccc}
5 & 0 & 0 \\
0 & 3 & 0
\end{array}\right)\left(\begin{array}{rrr}
1 / \sqrt{2} & 1 / \sqrt{2} & 0 \\
1 / \sqrt{18} & -1 / \sqrt{18} & 4 / \sqrt{18} \\
2 / 3 & -2 / 3 & -1 / 3
\end{array}\right)</script><p>上述所说的只是基本应用，下面我对SVD的公式进行基本的变换：</p>
<script type="math/tex; mode=display">\begin{array}{l}
A=U S V^{T} \\
A V=U S
\end{array}</script><p>也就是说SVD认为矩阵A是其所有奇异值与其对应的奇异向量的组合：</p>
<script type="math/tex; mode=display">A=\sigma_{1} u_{1} v_{1}^{T}+\ldots+\sigma_{r} u_{r} v_{r}^{T}</script><p>上述公式是理解SVD矩阵分解的关键，它提供了一种重要的方法去拆解m*n数据矩阵至r个元素，现在我们在来重新分析下之前的例子是如何工作的：</p>
<script type="math/tex; mode=display">A=\left(\begin{array}{ccc}
3 & 2 & 2 \\
2 & 3 & -2
\end{array}\right)</script><p>矩阵A可以被拆解为：</p>
<script type="math/tex; mode=display">\begin{aligned}
&=5\left(\begin{array}{ccc}
1 / \sqrt{2} \\
1 / \sqrt{2}
\end{array}\right)\left(\begin{array}{cccc}
1 / \sqrt{2} & 1 / \sqrt{2} & 0
\end{array}\right)+3\left(\begin{array}{cc}
1 / \sqrt{2} \\
-1 / \sqrt{2}
\end{array}\right)\left(\begin{array}{ccc}
1 / \sqrt{18} & -1 / \sqrt{18} & 4 / \sqrt{18}
\end{array}\right)\\
&=\left(\begin{array}{ccc}
3 & 2 & 2 \\
2 & 3 & -2
\end{array}\right)
\end{aligned}</script><h2 id="奇异值分解应用"><a href="#奇异值分解应用" class="headerlink" title="奇异值分解应用"></a>奇异值分解应用</h2><h3 id="moore-penrose伪逆"><a href="#moore-penrose伪逆" class="headerlink" title="moore-penrose伪逆"></a>moore-penrose伪逆</h3><p>对于线性方程组而言，我们往往需要计算矩阵的逆，如下图所示：</p>
<script type="math/tex; mode=display">\begin{aligned}
A x &=b \\
x &=A^{-1} b
\end{aligned}</script><p>但是对于大部分矩阵而言，都没有办法求其逆矩阵，因此在求解方程组时需要逆矩阵的替代品，moore-penrose伪逆提供了这样的一种方法，可以计算矩阵A的伪逆值，使得$ \left|A A^{+}-I_{n}\right|_{2} $ 最小，也就是伪逆矩阵$A^{+}$的相关性质最接近其逆矩阵，因此线性方程组的解就可以被估计为：</p>
<script type="math/tex; mode=display">\begin{aligned}
A x &=b \\
x & \approx A^{+} b
\end{aligned}</script><p>其中，为逆矩阵$A^{+}$的具体求解方式如下：</p>
<script type="math/tex; mode=display">\begin{array}{l}
A x=b \\
(U, D, V) \leftarrow \operatorname{svd}(A) \\
A^{+}=V D^{+} U^{T} \\
x=A^{+} b
\end{array}</script><p>我们以一个简单的例子来具体说明：<br>$A=\left[\begin{array}{ll}1 &amp; 1 \\ 1 &amp; 1\end{array}\right] \quad$ </p>
<p>$A=U \Sigma V^{T}=\frac{1}{\sqrt{2}}\left[\begin{array}{cc}1 &amp; 1 \\ 1 &amp; -1\end{array}\right]\left[\begin{array}{cc}2 &amp; 0 \\ 0 &amp; 0\end{array}\right] \frac{1}{\sqrt{2}}\left[\begin{array}{cc}1 &amp; 1 \\ 1 &amp; -1\end{array}\right]$</p>
<p>$A^{+}=V \Sigma^{+} U^{T}=\frac{1}{\sqrt{2}}\left[\begin{array}{cc}1 &amp; 1 \\ 1 &amp; -1\end{array}\right]\left[\begin{array}{cc}1 / 2 &amp; 0 \\ 0 &amp; 0\end{array}\right] \frac{1}{\sqrt{2}}\left[\begin{array}{cc}1 &amp; 1 \\ 1 &amp; -1\end{array}\right]=\frac{1}{4}\left[\begin{array}{cc}1 &amp; 1 \\ 1 &amp; 1\end{array}\right]$</p>
<p>&emsp;&emsp;moore-penrose伪逆的求解是基于奇异值分解的，通过上面的例子我们可以看出即便方阵不可逆，我们也能通过奇异值分解的方法得到其伪逆矩阵，也就是说该方法对于任意维度的任意矩阵都能适用。此外，待求矩阵A 的列数如果大于行数，得到就是所有可行解中L2范数最小的一个，如果相反，就是L2(Ax−y)最小</p>
<h3 id="数据压缩"><a href="#数据压缩" class="headerlink" title="数据压缩"></a>数据压缩</h3><p>&emsp;&emsp;在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵:</p>
<script type="math/tex; mode=display">A_{m \times n}=U_{m \times m} \Sigma_{m \times n} V_{n \times n}^{T} \approx U_{m \times k} \Sigma_{k \times k} V_{k \times n}^{T}</script><p>其中k要比n小很多，也就是一个大的矩阵A可以用三个小的矩阵$U_{m \times k}, \sum_{k \times k}, V_{k \times n}^{T}$来表示,这样就可以以舍弃部分精度来减少数据量。如下图所示，现在我们的矩阵A只需要灰色的部分的三个小矩阵就可以近似描述了。<br><img src="/2020/08/27/svd-pca/4.png" alt><br>由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪，PCA具体原理可以参考<a href="https://www.zhihu.com/question/41120789/answer/481966094"><strong>如何通俗易懂地讲解什么是 PCA 主成分分析</strong></a>。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引（LSI）。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>入门</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>基于SVD的图片分解</title>
    <url>/2020/08/28/svd-program/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇python基于numpy包实现SVD，因为有现成的API，所以直接调用即可。本代码实现对图片的奇异值分解效果展示</p>
<a id="more"></a>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">用户手册-by clever_bobo</span></span><br><span class="line"><span class="string">本函数用于测试奇异矩阵对图片进行分解，暂未设置任何纠错设置，请按照对应参数进行输入</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#读取文件，方块图像与风景图像</span></span><br><span class="line">im_square=mpimg.imread(<span class="string">r&quot;C:\Users\97751\Desktop\SVD\test1.png&quot;</span>)</span><br><span class="line">im_nature=mpimg.imread(<span class="string">r&quot;C:\Users\97751\Desktop\SVD\test2.png&quot;</span>)</span><br><span class="line"><span class="comment">#读取图片大小</span></span><br><span class="line">s1,s2,s3=im_square.shape</span><br><span class="line">n1,n2,n3=im_nature.shape</span><br><span class="line"><span class="comment">#转换格式</span></span><br><span class="line">im_square_temp = im_square.reshape(s1,s2 * s3)</span><br><span class="line">im_nature_temp = im_nature.reshape(n1, n2 * n3)</span><br><span class="line"><span class="comment">#调用numpy库中的线性函数的库</span></span><br><span class="line">U1,S1,VT1=np.linalg.svd(im_square_temp)</span><br><span class="line">U2,S2,VT2=np.linalg.svd(im_nature_temp)</span><br><span class="line"><span class="comment">#设置标志位</span></span><br><span class="line">flag=<span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> flag!=<span class="number">1</span>:</span><br><span class="line">    <span class="comment">#设置精度</span></span><br><span class="line">    nums=int(input(<span class="string">&quot;请输入保留的多少位奇异值：&quot;</span>))   </span><br><span class="line">    <span class="comment">#对方块图形进行SVD分解</span></span><br><span class="line">    img_square = (U1[:,<span class="number">0</span>:nums]).dot(np.diag(S1[<span class="number">0</span>:nums])).dot(VT1[<span class="number">0</span>:nums,:])</span><br><span class="line">    img_square= img_square.reshape(s1, s2 , s3)</span><br><span class="line">    <span class="comment">#对风景图形进行SVD分解</span></span><br><span class="line">    img_nature = (U2[:,<span class="number">0</span>:nums]).dot(np.diag(S2[<span class="number">0</span>:nums])).dot(VT2[<span class="number">0</span>:nums,:])</span><br><span class="line">    img_nature= img_nature.reshape(n1, n2 , n3)</span><br><span class="line">    <span class="comment">#精度分析</span></span><br><span class="line">    error1,error2=sum(S1[<span class="number">0</span>:nums])/sum(S1)*<span class="number">10000</span>//<span class="number">1</span>*<span class="number">0.01</span>,sum(S2[<span class="number">0</span>:nums])/sum(S2)*<span class="number">10000</span>//<span class="number">1</span>*<span class="number">0.01</span></span><br><span class="line">    <span class="comment">#画图部分</span></span><br><span class="line">    fig,ax=plt.subplots(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">    <span class="comment">#原始方块输出</span></span><br><span class="line">    ax[<span class="number">0</span>][<span class="number">0</span>].imshow(im_square)</span><br><span class="line">    ax[<span class="number">0</span>][<span class="number">0</span>].set(title = <span class="string">&quot;original square&quot;</span>)</span><br><span class="line">    <span class="comment">#SVD方块输出</span></span><br><span class="line">    ax[<span class="number">0</span>][<span class="number">1</span>].imshow(img_square)</span><br><span class="line">    ax[<span class="number">0</span>][<span class="number">1</span>].set(title = <span class="string">&quot; SVD&#x27;s square  precision=&quot;</span>+str(error1)+<span class="string">&#x27;%&#x27;</span>)</span><br><span class="line">    <span class="comment">#原始山脉输出</span></span><br><span class="line">    ax[<span class="number">1</span>][<span class="number">0</span>].imshow(im_nature)</span><br><span class="line">    ax[<span class="number">1</span>][<span class="number">0</span>].set(title = <span class="string">&quot;original nature&quot;</span>)</span><br><span class="line">    <span class="comment">#SVD山脉输出</span></span><br><span class="line">    ax[<span class="number">1</span>][<span class="number">1</span>].imshow(img_nature)</span><br><span class="line">    ax[<span class="number">1</span>][<span class="number">1</span>].set(title = <span class="string">&quot; SVD&#x27;s nature precision=&quot;</span>+str(error2)+<span class="string">&#x27;%&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    flag=int(input(<span class="string">&quot;是否结束SVD分解？是请输入1: &quot;</span>))</span><br><span class="line">print(<span class="string">&quot;感谢使用测试——by clever_bobo!!!&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="效果展示"><a href="#效果展示" class="headerlink" title="效果展示"></a>效果展示</h2><p>保留1位奇异值：<br><img src="/2020/08/28/svd-program/1.png" alt><br>保留5位奇异值：<br><img src="/2020/08/28/svd-program/2.png" alt><br>保留10位奇异值：<br><img src="/2020/08/28/svd-program/3.png" alt><br>保留50位奇异值：<br><img src="/2020/08/28/svd-program/4.png" alt><br>保留100位奇异值：<br><img src="/2020/08/28/svd-program/5.png" alt></p>
<p><strong>保留的奇异值位数越多，保留的信息就越多</strong></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>实例</tag>
      </tags>
  </entry>
</search>
