<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>深度理解逻辑回归</title>
    <url>/2020/08/27/log-regression/</url>
    <content><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>&emsp;&emsp;所谓机器学习，听起来很高深莫测，实际上并不复杂，简单来说就是数据驱动的算法。而数据驱动就是指在给定的数据情况下，我们需要找到一种合适的算法对这些数据进行操作，从而实现我们预期的目标，具体我们需要做的任务就是找到合适的模型来描述输入到输出之间的映射关系，然后在使用优化的方法不断对模型中的参数进行优化，使得最后得到的结果鲁棒性最高。在上述的描述中，主要涉及两个方面：1.合适的模型 2.合适的优化算法。其实，我们在初中就接触过相关的领域，比如给定一系列坐标点（x，y），利用线性回归公式（最小二乘法）拟合曲线y=ax+b。其中，y=ax+b就是我们选择的模型，而最小二乘法就是对其参数a，b的优化算法。当然，这只是最简单的应用，实际机器学习的算法往往要比这个复杂很多，但究其本质还是一样的。<br>&emsp;&emsp;对于机器学习而言，其功能非常强大，能完成分类、回归、转录、机器翻译、异常与检测、合成和采样等等。其中，分类与回归是机器学习的最基本的两项功能。上述举的例子就是回归算法。而今天，我主要介绍一种常用的分类算法——逻辑回归。对于分类的而言，逻辑回归是一种重要的学习方式，该方法所假设的函数在经过优化后的到模型适用性非常高。另外，之所以该方法在名称中带有回归二字，是因为其算法原理与线性回归之间有很深的联系，在下面介绍算法原理的时候我会重点讲解。</p>
<a id="more"></a>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>&emsp;&emsp;分类，顾名思义，就是将输入的数据分为不同的类别，其结果是离散的，比如预测明天的天气是去晴天还是非晴天。在计算机中，我们往往使用不同的数字代表不用的类别，比如1代表晴天，0代表非晴天。<br>&emsp;&emsp;但是，如果算法仅仅只能够告诉我们分类的结果是非常不精准的，我们更期望算法能够告诉我们发生某种情况的概率，比如明天晴天的概率为80%，非晴天的概率为20%，这样的话我们的可操作性就会更强，可以人为的添加参数（优化）对算法进行矫正。仍以晴天为例，如果分类算法较为精准，我们可以用50%为阈值，如果晴天的概率大于50%，就是晴天；如果算法计算晴天概率比实际偏大，那我们可以设置60%为阈值，大于60%为晴天</p>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>&emsp;&emsp;在正式讲解逻辑回归的算法之间，首先我们需要了解一下线性回归的基本原理。我们假定输出y由输入x线性决定（这里的x,y都是向量)，其表达式为：</p>
<script type="math/tex; mode=display">f(x)=\theta^{T} x······（1）</script><p>当输入变量只有一个时，就变成了我们熟悉的y=ax+b，此时，利用最小二乘法，具体的效果大致如下：<br><img src="/2020/08/27/log-regression/1.png" alt="y=ax+b"><br>&emsp;&emsp;当多个变量的时候，就由一维向量到多维，例如二维就会得到一个平面，而非一条直线了，其共同的特点就是模型是连续的，其值域为（-∞，+∞），这是线性回归模型的一个重要的特征。</p>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>&emsp;&emsp;逻辑回归是一种回归算法，该种算法主要应用于二分类的状况，例如明天北京是否会下雪，一个人五年内是否会得心脏病等等，由线性回归延伸出来的，那么具体是如何出现的呢？下面我来具体说明，仍以晴天为例：</p>
<ol>
<li>对于明天天气如何，我们该如何预测呢？首先，对于给定输入，比如温度，云层厚度，时间，风力，这些参数在算法中表现为x1,x2,x3……，我们最容易想到的就是令预测概率p（x）为x的线性函数，这样就和线性回归一致了，其值域为（-∞，+∞），并不符合我们的要求（概率应该在0~1之间），因此我们需要对其进行改进，使其符合我们的要求；</li>
<li>如果我们需要限制值域，在机器学习中最常用的就是ln函数，因此我们做一个简单的调整，令lnp(x)为x的线性函数，也就是说p（x）=exp(ax+b)，但是该函数无论正负，均只能在一个方向上约束值域，因此还需要改进；</li>
<li>最后，对lnp作简单的调整，令其在两个方向都被约束，我们用的方法是逻辑转换，令ln(p/1-p)为x的线性函数，那么p（x）的值域就是[0,1]<br>&emsp;&emsp;因此逻辑回归的表达式为（在印刷体中我们往往都采用log代表ln）：<script type="math/tex; mode=display">\log \frac{p(x)}{1-p(x)}=\beta_{0}+x \beta··········（2）</script></li>
</ol>
<p>如（2）中所示，p（x）为事情发生的概率，令$\log \frac{p(x)}{1-p(x)}$成为x的线性函数，解得p(x)为：</p>
<script type="math/tex; mode=display">\mathrm{p}\left(x ; \beta_{0}, \beta\right)=\frac{1}{1+e^{-\left(\beta_{0}+x \beta\right)}}··········（3）</script><p>（3）式相比于（2）式更容易理解条件概率p（x），但（2）式更能凸显逻辑回归与线性回归之间的关系。<br>如图所示：<br><img src="/2020/08/27/log-regression/2.png" alt="逻辑回归"><br>该图为函数逻辑回归曲线的大致形状，基本完成了我们期望的条件：曲线较为平滑，值域位于[0,1]。</p>
<h2 id="主要特征"><a href="#主要特征" class="headerlink" title="主要特征"></a>主要特征</h2><ol>
<li>一般取$\beta_{0}+x \quad \beta=0$为分类的边界，那么如果x是一维，那么分类边界就是一个点（类似在数轴上分类）；如果是二维分类边界就是一条直线，以此类推。之所以如此设置，是因为设置β0+ x β= 0为边界后，我们可以认为当算法输出的概率p（x）≥0.5时，分类结果 Y为1,；当p&lt;0.5时，分类结果Y为0。或者说算法输入的$\beta_{0}+x \quad \beta≥0$时，分类结果为1；$\beta_{0}+x \quad \beta&lt;0$时，分类结果为0（目前只考虑二分类的情况，多种分类情况在后面有介绍）。这样我们就可以把前面计算得到的概率转换为分类的结果了，既得到了分类的条件概率，又得到了分类的结果。</li>
<li>逻辑回归计算得到的条件概率是由数据点到边界之间的距离决定的，为$\frac{\beta_{0}+x \beta}{|\beta|}$。也就是说如果距离边界远，那么为1（或者0）的概率就会越大。另外这个公式也说明了当||β||越大时，在同一数据集下，分类得到的概率会更像极端（0,1）靠近，如下图所示<br><img src="/2020/08/27/log-regression/3.png" alt="逻辑回归"><br>（备注：最后一种是用线性分类的方式进行分类的）</li>
<li>逻辑回归是跟据线性回归演化而来，提出时间早，科学家们对其研究较为透彻，运用较为熟练。此外，该算法较为简单，且适用性较强，机器学习的算法在准确度够得情况下，尽量选择较为简单的算法，避免出现过拟合</li>
</ol>
<h2 id="对于多分类的逻辑回归"><a href="#对于多分类的逻辑回归" class="headerlink" title="对于多分类的逻辑回归"></a>对于多分类的逻辑回归</h2><p>逻辑回归本质上是对二分类的事件进行的分类，但是对于多分类的情况，我们可以通过建立多个分类模型，将多分类差分成多个二分类的情况，在利用逻辑回归进行分类，如图所示：<br><img src="/2020/08/27/log-regression/4.png" alt="逻辑回归"><br>一般而言如果存在n中情况，我们会设置n个分类器，每个分类器对其中一种进行识别，其条件概率的计算公式为：</p>
<script type="math/tex; mode=display">P(Y=c \mid X=x)=\frac{e^{\beta_{0}^{(c)}+x \beta^{(c)}}}{\sum_{i=1}^{n} e^{\beta_{0}^{(j)}+x \beta^{(i)}}}</script><p>判别方法与之前类似，如果超过0.5就认为是第c类。值得一提是，当只有两种情况时，令$\beta^{0}=\beta_{0}^{(1)}-\beta_{0}^{(0)}$以及$\beta=\beta^{(1)}-\beta^{(0)}$时，多情况的逻辑分类就转换成二分类的情况了。<br><br>下面是我自己的推导过程：<br><img src="/2020/08/27/log-regression/5.png" alt="逻辑回归"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>入门</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>markdown基本语法一</title>
    <url>/2020/08/26/markdown1/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇blog主要面向初步接触markdown的小白，主要是面向windows用户，简单介绍markdown语法规则，同时方便自己在忘记时能快速查看~<br>本文参考文献：<br><a href="https://hyxxsfwy.github.io/2016/01/15/Hexo-Markdown-%E7%AE%80%E6%98%8E%E8%AF%AD%E6%B3%95%E6%89%8B%E5%86%8C">一、简明语法手册</a><br><a href="https://www.mereith.com/2018/12/08/markdown%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/">二、某知名哲学家的blog</a><br><a href="https://www.jianshu.com/p/191d1e21f7ed/">三、Markdown基本语法</a><br><a href="https://www.jianshu.com/p/599857933f6e">四、Markdown基本语法总结</a></p>
<h2 id="创建新文章"><a href="#创建新文章" class="headerlink" title="创建新文章"></a>创建新文章</h2><p>第一种：打开powershell（管理员），打开到对应位置的目录，输入以下代码<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo new <span class="string">&quot;新文章的名字&quot;</span></span><br></pre></td></tr></table></figure><br>第二种：打开blog文章所在的位置，例如<code>D:\Program Files\hexo-blog\myblog\source\_posts</code>,创建txt文件，更改拓展名为.md</p>
<p>两种方法没有什么本质区别，效率也是一样的，大家可以根据自己的习惯选择,个人推荐选择第一种~</p>
<h2 id="创建blog文件的文件头部分"><a href="#创建blog文件的文件头部分" class="headerlink" title="创建blog文件的文件头部分"></a>创建blog文件的文件头部分</h2><p>每一篇blog一般都要包含4个部分，以本篇为例：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">title: markdown基本语法一</span><br><span class="line">date: 2020-08-26 23:00:00</span><br><span class="line">tags: [markdown,入门] </span><br><span class="line">categories: markdown</span><br><span class="line">description: 本篇blog主要面向初步接触markdown的小白，主要是面向windows用户，简单介绍markdown语法规则，同时方便自己在忘记时能快速查看~</span><br></pre></td></tr></table></figure><br>分别对应本篇文章的标题，编辑时间，标签以及文章分类。另外，标签可以有多个，用英文逗号隔开；分类建议只有一个；简介只在首页列表显示，打开后不会显示，是可选项。<br><a id="more"></a></p>
<h2 id="编写文章的标题"><a href="#编写文章的标题" class="headerlink" title="编写文章的标题"></a>编写文章的标题</h2><p>markdown支持6级标题，以<code>#</code>作为关键字，识别标题，实例如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 一级标题</span></span><br><span class="line"><span class="comment">## 二级标题</span></span><br><span class="line"><span class="comment">### 三级标题</span></span><br><span class="line"><span class="comment">#### 四级标题</span></span><br><span class="line"><span class="comment">##### 五级标题</span></span><br><span class="line"><span class="comment">###### 六级标题</span></span><br></pre></td></tr></table></figure><br>效果如下：<br><img src="https://pic.mereith.com/img/show_title.png-slim" alt></p>
<p>此外还有一些特殊的标题符号，例如在标题前加入<code>-</code>, <code>*</code>,<code>+</code>，可着重标记该项，效果如下：</p>
<ul>
<li>着重强调符号</li>
</ul>
<h2 id="字体设置"><a href="#字体设置" class="headerlink" title="字体设置"></a>字体设置</h2><h3 id="粗体和斜体"><a href="#粗体和斜体" class="headerlink" title="粗体和斜体"></a>粗体和斜体</h3><p>使用<code>*</code>和<code>**</code>分别表示斜体和粗体，格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">*斜体*，**粗体**，***斜体加粗***</span><br></pre></td></tr></table></figure><br>效果展示：<em>斜体</em>,<strong>粗体</strong>,<strong><em>斜体加粗</em></strong></p>
<h2 id="字体、字号、颜色"><a href="#字体、字号、颜色" class="headerlink" title="字体、字号、颜色"></a>字体、字号、颜色</h2><p>使用关键字，可以指定字体的颜色和大小，其格式为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">指定字体类型： &lt;font face=<span class="string">&quot;黑体&quot;</span>&gt;我是黑体字&lt;/font&gt;</span><br><span class="line">指定字体大小： &lt;font size=12&gt;我是12号字&lt;/font&gt;</span><br><span class="line">指定字体颜色：&lt;font color=<span class="comment">#0099ff&gt;我是蓝色字&lt;/font&gt; #0099ff 为颜色的16进制代码</span></span><br><span class="line">指定字体颜色、字号、字体类型&lt;font color=<span class="comment">#0099ff size=12 face=&quot;黑体&quot;&gt;黑体&lt;/font&gt;</span></span><br></pre></td></tr></table></figure><br>效果如下：<br>指定字体类型： <font face="黑体">我是黑体字</font><br>指定字体大小： <font size="12">我是12号字</font><br>指定字体颜色：<font color="#0099ff">我是蓝色字</font> #0099ff 为颜色的16进制代码<br>指定字体颜色、字号、字体类型<font color="#0099ff" size="12" face="黑体">黑体</font></p>
<h3 id="换行"><a href="#换行" class="headerlink" title="换行"></a>换行</h3><p>方法1：连敲2个以上空格+enter键；<br>方法2：利用html语法，<code>&lt;br&gt;</code>。</p>
<h3 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h3><p>单独一行使用<code>***</code>或者<code>---</code>，表示该行作为分割线</p>
<hr>
<h3 id="删除线"><a href="#删除线" class="headerlink" title="删除线"></a>删除线</h3><p>删除线是指在原文本上画一条线，类似在纸上写错了划线来删除，一般用于表示过时的版本或者错误的写法较为醒目，格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">~~删除线~~</span><br></pre></td></tr></table></figure><br>效果：<del>删除线</del></p>
<h3 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h3><p>在编写blog的时候，往往会参考一些网站，我们需要把网站链接放在正文中，此时就需要用到超链接；此外还可以放一些图片链接<br>网站链接格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[链接说明](链接地址)</span><br></pre></td></tr></table></figure><br>例如：<a href="https://www.mereith.com/">某知名哲学家♂的个人主页</a></p>
<p>图片超链接格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">![图片说明](图片地址)</span><br></pre></td></tr></table></figure><br>例如：<img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=543966357,2530364137&amp;fm=26&amp;gp=0.jpg" alt="某知名老婆的图片" title="蕾姆"></p>
<h3 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h3><p>可以用<code>\</code>来表示注释，也就是说<code>\</code>后的文字不会被转义，格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">\<span class="comment">#标题格式，但不是标题</span></span><br></pre></td></tr></table></figure><br>具体效果如下：#标题格式，但不是标题</p>
<h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><p>使用<code>&gt;</code>来表示文字的引用，往往在引用参考文献中的语句时使用，其格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; Sow nothing, reap nothing</span><br></pre></td></tr></table></figure><br>此外，引用还可以嵌套使用，例如:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; 这是第一个引用</span><br><span class="line">&gt;&gt; 这是第一个引用中的引用</span><br><span class="line">&gt;&gt;&gt; 这是第一个引用中的引用的引用</span><br></pre></td></tr></table></figure><br>效果展示如下：</p>
<blockquote>
<p>这是第一个引用</p>
<blockquote>
<p>这是第一个引用中的引用</p>
<blockquote>
<p>这是第一个引用中的引用的引用<br>我就不套娃了，大家自己可以试试~</p>
<h3 id="代码块"><a href="#代码块" class="headerlink" title="代码块"></a>代码块</h3><p><strong>首先</strong>，是行内代码块，使用一对 <code>来括住文字，格式如下：
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">行内`代码`块</span><br></pre></td></tr></table></figure>
其效果如下：行内</code>代码`块</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>其次</strong>，多行代码块，使用一对```来括住文字，效果如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第一行</span><br><span class="line">第二行</span><br><span class="line">第三行</span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<p><strong>最后</strong>，支持规定语言的代码展示，以python为例，在 ``` 后加入编码格式即可<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(<span class="string">&quot;hello world&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="引用本地图片"><a href="#引用本地图片" class="headerlink" title="引用本地图片"></a>引用本地图片</h2><p>在hexo中，如果你想引用本地图片，最好先安装hexo-asset-image的插件，保证图片不会因为移动而丢失。安装完该插件之后，每次产生新的.md文件的同时，还会在生成与之同名的文件夹。将想要上传的图片传入该文件夹，按照<code>![你想输入的替代文字](xxxx/图片名.jpg)</code>引用即可，效果如下：<br><img src="/2020/08/26/markdown1/test1.jpg" alt="测试图片"></p>
]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>入门</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>markdown进阶心得（更新中）</title>
    <url>/2020/08/27/markdown2/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇blog主要是在编辑markdown时，遇到了很多问题，同时也找到了很多技巧，所以在这里mark一下</p>
<h2 id="首行缩进"><a href="#首行缩进" class="headerlink" title="首行缩进"></a>首行缩进</h2><p>英文字符空格 <code>&amp;ensp;</code><br>中文字符空格 <code>&amp;emsp;</code><br>不断行的空白格 <code>&amp;nbsp;</code><br>其中较为常用的是<code>&amp;emsp;</code>,其效果如下：<br>&emsp;&emsp;<strong>首行缩进两字符</strong><br><a id="more"></a></p>
<h2 id="文字居中"><a href="#文字居中" class="headerlink" title="文字居中"></a>文字居中</h2><p>文字居中使用<code>&lt;center&gt; ``&lt;/center&gt;</code>来括住文字，格式如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;center&gt; 文字居中&lt;&#x2F;center&gt; </span><br></pre></td></tr></table></figure><br>效果展示：</p>
<center> 文字居中</center> 

<h2 id="插入公式"><a href="#插入公式" class="headerlink" title="插入公式"></a>插入公式</h2><p>参考链接：(<a href="https://www.jianshu.com/p/7ab21c7f0674">https://www.jianshu.com/p/7ab21c7f0674</a>)<br>按照参考链接里面的教程，将冲突的配置文件更改，即可在exo中渲染MathJax数学公式<br><br>但是该公式有严格的LaTeX语法，语法规则参考<a href="https://www.jianshu.com/p/25f0139637b7">《markdown中公式编辑教程》</a>,所以较为复杂，在这里推荐一种懒人方法：使用“mathpix”插件，官方的下载地址为(<a href="https://mathpix.com/)。">https://mathpix.com/)。</a><br>使用方法浅显易懂，注册账号登录，截屏选取公式，复制结果。如果是编辑公式，我们可以在word上编写或者写在纸上，用该软件转换格式，实测非常好用~</p>
<h2 id="writage插件"><a href="#writage插件" class="headerlink" title="writage插件"></a>writage插件</h2><p>官方地址：(<a href="https://www.writage.com">https://www.writage.com</a>)<br>使用教程：(<a href="https://www.cnblogs.com/craigtaylor/p/13540170.html">https://www.cnblogs.com/craigtaylor/p/13540170.html</a>)<br>该插件可以将word文件转换为.md文件，但是经过我的实测，并不是很理想，待开发中~</p>
<center> <font size="12" color="#DC143C" face="黑体">未完待续~</font> </center>

]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>进阶</tag>
      </tags>
  </entry>
  <entry>
    <title>通俗理解梯度下降法</title>
    <url>/2020/08/28/sgd/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>机器学习的本质就是模型+优化，本篇blog主要是简单介绍一种优化算法——梯度下降法。相比于其他的优化算法，该算法的适用性较高，尤其在深度网络的学习中，主流的优化算法就是梯度下降法。虽然说该方法适用性高，但是仍有一定的局限性，比如对于非凸函数收敛于局部最小，梯度消失，梯度爆炸等问题，但总的来说该算法的应用还是十分广泛的。</p>
<h2 id="场景假设"><a href="#场景假设" class="headerlink" title="场景假设"></a>场景假设</h2><p>&emsp;&emsp;首先，我们先假设一个场景：一个人被放在山上，现在他想找到一条下山的路到达山脚，但是这个人没有地图，也不知道所处位置和方向。另外山上还起了大雾，导致能见度很低，因此没有办法直接找到一条合适的路径，只能自己一步步摸索，那么这个时候，便可利用梯度下降算法来帮助自己下山。<br><img src="/2020/08/28/sgd/1.jpg" alt><br>&emsp;&emsp;具体怎么做呢，首先以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着下降方向走一步，然后又继续以当前位置为基准，再找最陡峭的地方，再走直到最后到达最低处。虽然这么走不一定是最短路径，但是每一步都能保证自己离山脚更进一步，这就是梯度下降法的核心：一步步慢慢的靠近最小值点，不一定最快，但一定有效。那么一定会有同学问了，如果我想找到f(x)的最高点怎么办？同样，我们可以利用梯度下降的方法找-f(x)最小点即可。</p>
<a id="more"></a>
<h2 id="数学基础知识回顾"><a href="#数学基础知识回顾" class="headerlink" title="数学基础知识回顾"></a>数学基础知识回顾</h2><h3 id="一元函数的导数与泰勒展开"><a href="#一元函数的导数与泰勒展开" class="headerlink" title="一元函数的导数与泰勒展开"></a>一元函数的导数与泰勒展开</h3><p>&emsp;&emsp;在微积分中, 函数 $f(x)$ 在点 $x_{0}$ 上的导数定义为:</p>
<script type="math/tex; mode=display">\quad f^{\prime}\left(x_{0}\right)=\lim _{x \rightarrow x_{0}} \frac{f(x)-f\left(x_{0}\right)}{x-x_{0}}</script><p>&emsp;&emsp;它在几何上指的就是函数 $f(x)$ 在 $x_{0}$ 上的切线方向.通常来说，为了计算某个函数 $f(x)$ 的最大值或者最小值，通常都会计算它的导数 $f^{\prime}(x)$ ，然后求解方程 $f^{\prime}(x)=0$ 就可以得到函数的临界点，进一步判断这些临界点是否是最大值或者最 小值。但是临界点并不一定是全局最大值或者全局最小值，基至不是局部的最大值或者局部最小值。<br><br>例如：函数 $f(x)=x^{3},$ 它的导数是 $f^{\prime}(x)=3 x^{2},$ 因此 $x=0$ 是它的临界点。但 $x=0$ 则不是这个函数的局部最大值或者局部最小值点, 因为 $f(x)<0, \forall x<0$ 且 $f(x)>0, \forall x&gt;0$。从 Taylor 级数的角度来看, $\quad f(x)$ 在 $x_{0}$ 附近的 Taylor 级数是</0,></p>
<script type="math/tex; mode=display">
f(x)=f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\frac{f^{\prime \prime}\left(x_{0}\right)}{2}\left(x-x_{0}\right)^{2}+O\left(\left|x-x_{0}\right|^{3}\right)</script><p>&emsp;&emsp;对于临界点 $x_{0}$ 而言，它满足条件 $f^{\prime}\left(x_{0}\right)=0$ 。当 $f^{\prime \prime}\left(x_{0}\right)&gt;0$ 时, 可以得到 $x_{0}$ 是 $f(x)$ 的局部最小值;当$f^{\prime \prime}\left(x_{0}\right)&lt;0$时, 可以得到 $x_{0}$ 是 $f(x)$ 的局部最大值。而对于上面的例子 $f(x)=x^{3}$ 而言，临界点 0 的二阶导数则是 $f^{\prime \prime}(0)=0$, 因此使用上面的方法则无法判断临界点 0 是否是局部极值。</p>
<h3 id="多元函数的梯度与泰勒展开"><a href="#多元函数的梯度与泰勒展开" class="headerlink" title="多元函数的梯度与泰勒展开"></a>多元函数的梯度与泰勒展开</h3><p>&emsp;&emsp;对于多元函数 $f(\mathbf{x})=f\left(x_{1}, \cdots, x_{n}\right)$ 而言，同样可以计算它们的”导数”，也就是偏导数和梯度。i.e. 它的梯度可以定义为：</p>
<script type="math/tex; mode=display">
\nabla f(\mathbf{x})=\left(\frac{\partial f}{\partial x_{1}}(\mathbf{x}), \cdots, \frac{\partial f}{\partial x_{n}}(\mathbf{x})\right)</script><p>而多元函数 $f(\mathbf{x})$ 在点 $\mathbf{x}_{0}$ 上的 Taylor 级数是：</p>
<script type="math/tex; mode=display">f(\mathbf{x})=f\left(\mathbf{x}_{0}\right)+\nabla f\left(\mathbf{x}_{0}\right)\left(\mathbf{x}-\mathbf{x}_{0}\right)+\frac{1}{2}\left(\mathbf{x}-\mathbf{x}_{0}\right)^{T} H\left(\mathbf{x}-\mathbf{x}_{0}\right)+O\left(\left|\mathbf{x}-\mathbf{x}_{0}\right|^{3}\right)</script><p>其中 $H$ 表示 Hessian 矩阵。如果 $x_{0}$ 是临界点，并且 Hessian 矩阵是正定矩阵的时候, $f(\mathbf{x})$ 在 $x_{0}$ 处达到局部极小值。</p>
<h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><p>&emsp;&emsp;从数学上的角度来看，梯度的方向是函数增长速度最快的方向，那么梯度的反方向就是函数减少最快的方向。那么，如果想计算一个函数的最小值，就可以使用梯度下降法的思想来做。<br>&emsp;&emsp;假设希望求解目标函数 $f(\mathbf{x})=f\left(x_{1}, \cdots, x_{n}\right)$ 的最小值, 可以从一个初始点 $\mathbf{x}^{(0)}=\left(x_{1}^{(0)}, \cdots, x_{n}^{(0)}\right)$ 开始, 基于学习率 $\eta&gt;0$ 构建一个迭代过程：当 $i \geq 0$ 时</p>
<script type="math/tex; mode=display">
x_{1}^{(i+1)}=x_{1}^{(i)}-\eta \cdot \frac{\partial f}{\partial x_{1}}\left(\mathbf{x}^{(i)}\right)</script><script type="math/tex; mode=display">
··········</script><script type="math/tex; mode=display">
x_{n}^{(i+1)}=x_{n}^{(i)}-\eta \cdot \frac{\partial f}{\partial x_{n}}\left(\mathbf{x}^{(i)}\right)</script><p>其中 $\mathbf{x}^{(i)}=\left(x_{1}^{(i)}, \cdots, x_{n}^{(i)}\right),$ 一旦达到收禽条件的话, 迭代就结束。<br><br>&emsp;&emsp;从梯度下降法的迭代公式来看, 下一个点的选择与当前点的位置和它的梯度相关。反之，如果要计算函数 $f(\mathbf{x})=f\left(x_{1}, \cdots, x_{n}\right)$ 的最大值, 沿着梯度的反方向前进即可,也就是说：</p>
<script type="math/tex; mode=display">x_{1}^{(i+1)}=x_{1}^{(i)}+\eta \cdot \frac{\partial f}{\partial x_{1}}\left(\mathbf{x}^{(i)}\right)</script><script type="math/tex; mode=display">··········</script><script type="math/tex; mode=display">x_{n}^{(i+1)}=x_{n}^{(i)}+\eta \cdot \frac{\partial f}{\partial x_{n}}\left(\mathbf{x}^{(i)}\right)</script><p>其中, $\mathbf{x}^{(i)}=\left(x_{1}^{(i)}, \cdots, x_{n}^{(i)}\right)$ 。整体来看，无论是计算函数的最大值或者最小值，都需要构建 一个迭代关系 $g$ ，那就是：</p>
<script type="math/tex; mode=display">\mathbf{x}^{(0)} \stackrel{g}{\longrightarrow} \mathbf{x}^{(1)} \stackrel{g}{\longrightarrow} \mathbf{x}^{(2)} \stackrel{g}{\longrightarrow} \cdots</script><p>也就是说对于所有的 $i \geq 0,$ 都满足迭代关系 $x^{(i+1)}=g\left(x^{(i)}\right)$ 。所以, 在以上的两个方法 中，我们可以写出函数 g 的表达式为：</p>
<script type="math/tex; mode=display">g(\mathbf{x})=\left\{\begin{array}{ll}
\mathbf{x}-\eta \nabla f(\mathbf{x}) & \text { 梯度下降法 } \\
\mathbf{x}+\eta \nabla f(\mathbf{x}) & \text { 梯度上升法 }
\end{array}\right.</script><p>但在实际应用时，当我们求解最大值时，可以对函数取反，求其最小值。</p>
<h2 id="实例测试"><a href="#实例测试" class="headerlink" title="实例测试"></a>实例测试</h2><h3 id="单变量函数的梯度下降法"><a href="#单变量函数的梯度下降法" class="headerlink" title="单变量函数的梯度下降法"></a>单变量函数的梯度下降法</h3><p>在这么我们举一个简单的例子，对于目标函数：</p>
<script type="math/tex; mode=display">J(\theta)=\theta^{2}</script><p>对函数进行微分，直接求导就可以得到</p>
<script type="math/tex; mode=display">J^{\prime}(\theta)=2 \theta</script><p>初始化，也就是起点，起点可以随意的设置，这里设置为</p>
<script type="math/tex; mode=display">\theta^{0}=1</script><p>学习率也可以随意的设置，这里设置为0.4</p>
<script type="math/tex; mode=display">\alpha=0.4</script><p>根据梯度下降的计算公式</p>
<script type="math/tex; mode=display">\Theta^{1}=\Theta^{0}-a \nabla J(\Theta) \quad 初始值为\Theta^{0}</script><p>我们开始进行梯度下降的迭代计算过程:</p>
<script type="math/tex; mode=display">\begin{aligned} \theta^{0} &=1 \\ \theta^{1} &=\theta^{0}-\alpha * J^{\prime}\left(\theta^{0}\right) \\ &=1-0.4 * 2 \\ &=0.2 \\ \theta^{2} &=\theta^{1}-\alpha * J^{\prime}\left(\theta^{1}\right) \\ &=0.04 \\ \theta^{3} &=0.008 \\ \theta^{4} &=0.0016 \end{aligned}</script><p>在上面的计算过程中，初始参数$\theta^{0}$，学习率$\alpha$我们是随意选择的，但因为本次的例子较为简单，所以效果还不错。但在实际应用中，目标函数往往会十分复杂，对于初始值的选择也十分重要，在后面我会介绍如何选取合适的初始值。<br><img src="/2020/08/28/sgd/2.png" alt></p>
<h3 id="多变量函数的梯度下降"><a href="#多变量函数的梯度下降" class="headerlink" title="多变量函数的梯度下降"></a>多变量函数的梯度下降</h3><p>我们假设有一个目标函数</p>
<script type="math/tex; mode=display">
J(\Theta)=\theta_{1}^{2}+\theta_{2}^{2}</script><p>现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 (0，0)点。但是接下来，我们会从 梯度下降算法开始一步步计算到这个最小值！ 我们假设初始的起点为：</p>
<script type="math/tex; mode=display">\Theta^{0}=(1,3)</script><p>初始的学习率为：</p>
<script type="math/tex; mode=display">\alpha=0.1</script><p>函数的梯度为：</p>
<script type="math/tex; mode=display">\nabla J(\Theta)=\left\langle 2 \theta_{1}, 2 \theta_{2}\right\rangle</script><p>进行多次迭代：</p>
<script type="math/tex; mode=display">\begin{aligned}
\Theta^{0} &=(1,3) \\
\Theta^{1} &=\Theta^{0}-\alpha \nabla J(\Theta) \\
&=(1,3)-0.1(2,6) \\
&=(0.8,2.4) \\
\Theta^{2} &=(0.8,2.4)-0.1(1.6,4.8) \\
&=(0.64,1.92) \\
\Theta^{3} &=(0.512,1.536) \\
\Theta^{4} &=(0.4096,1.2288000000000001) \\
\vdots & \\
\Theta^{10} &=(0.10737418240000003,0.32212254720000005) \\
\vdots & \\
\Theta^{50} &=\left(1.1417981541647683 \mathrm{e}^{-05}, 3.425394462494306 \mathrm{e}^{-05}\right) \\
\vdots & \\
\Theta^{100} &=\left(1.6296287810675902 \mathrm{e}^{-10}, 4.888886343202771 \mathrm{e}^{-10}\right)
\end{aligned}</script><p><img src="/2020/08/28/sgd/3.png" alt></p>
<h2 id="梯度下降法的种类"><a href="#梯度下降法的种类" class="headerlink" title="梯度下降法的种类"></a>梯度下降法的种类</h2><h3 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h3><p>&emsp;&emsp;这是梯度下降法刚刚提出时的主流类型，但是现在已经不经常使用了，因为这种优化算法会使用整个数据集去计算代价函数的梯度去更新参数，如果数据集非常的大，批量梯度下降法会很慢，而且数据量这么大往往无法载入内存，但好处就是，在每次迭代完成之后能够保证每次的优化都是有效的。在随机初始化参数后，按如下方式计算代价函数的梯度，直到数据收敛：</p>
<script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}</script><p><img src="/2020/08/28/sgd/4.png" alt><br>上图是每次迭代后的等高线图，每个不同颜色的线表示代价函数不同的值。运用梯度下降会快速收敛到圆心，即唯一的一个全局最小值。</p>
<h3 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><p>&emsp;&emsp;相比于批量梯度下降法，随机梯度下降法具有更快的计算速度。首先，随机梯度下降的第一步是随机化整个数据集。在每次迭代仅选择其中一部分训练样本去计算代价函数的梯度，然后更新参数。即使是大规模数据集，随机梯度下降法也会很快收敛。由于数据点可能存在的误差，随机梯度下降得到结果的准确性可能不会是最好的，但是计算结果的速度很快。具体的计算方式如下：<br><img src="/2020/08/28/sgd/5.png" alt><br>其中，m代表训练样本的数量，如下图所示，随机梯度下降法不像批量梯度下降法那样收敛，而是游走到接近全局最小值的区域终止：<br><img src="/2020/08/28/sgd/6.png" alt></p>
<h3 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h3><p>&emsp;&emsp;小批量梯度下降法是最广泛使用的一种算法，该算法每次随机使用m个训练样本（称之为一批）进行训练，能够更快得出准确的答案。小批量梯度下降法不是使用完整数据集，在每次迭代中仅使用m个训练样本去计算代价函数的梯度。一般小批量梯度下降法所选取的样本数量在50到256个之间，视具体应用而定，具体的计算方式如下：<br><img src="/2020/08/28/sgd/7.png" alt><br>其中，b表示一批训练样本的个数，m是训练样本的总数。</p>
<h2 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h2><h3 id="非凸函数局部收敛"><a href="#非凸函数局部收敛" class="headerlink" title="非凸函数局部收敛"></a>非凸函数局部收敛</h3><p>如果想要利用梯度下降法找到目标函数的全局最小值，那么该函数必须是凸函数<a href="https://zhuanlan.zhihu.com/p/51127402">(参考链接：什么是凸函数)</a>,如果不是凸函数，那么利用梯度下降法就有可能收敛于局部最小值，如下图所示<br><img src="/2020/08/28/sgd/9.png" alt><br>函数最终会收敛在上面的点，而非我们需要的最小值点！但是在实际应用中，很多目标函数都是非凸函数，尤其是神经网络，那么我们该如何解决这个问题呢？据我所知，当前还没有完美的解决方案，一般采用合适的学习率或者搭配一些其他的优化算法，使得计算出来的结果（可能是局部最优解）仍具有良好的效果，从而用该值作为我们优化算法的最优值，可能在算法上讲的并不是很清楚，有点像是黑盒子，但事实证明确实很有效。</p>
<h3 id="学习率选取不合适"><a href="#学习率选取不合适" class="headerlink" title="学习率选取不合适"></a>学习率选取不合适</h3><p>在本文的演示的例子中，因为目标函数较为简单，所以在随便拿了一个学习率后，仍然有不错的效果，但是实际应用时，如果学习率选取不当，会造成严重的后果，如下图所示：<br><img src="/2020/08/28/sgd/8.png" alt><br>如果学习率选取过大，那么计算结果就会在收敛点处震荡，而无法收敛到最小值；如果学习率选的过小，那么计算时间就会过长，占用大量资源，同时还有可能陷入局部最小而无法跳出。</p>
<h3 id="梯度爆炸与梯度消失"><a href="#梯度爆炸与梯度消失" class="headerlink" title="梯度爆炸与梯度消失"></a>梯度爆炸与梯度消失</h3><p>&emsp;&emsp;梯度爆炸与梯度消失是神经网络经常出现的一个问题，但究其本质还是梯度下降法的局限性，在这里简单介绍下，后面涉及到神经网络时再具体讲解。对函数求导数是基于链式，而链式法则是一个连乘的形式，所以当层数越深的时候，梯度将以指数形式传播。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。在根据损失函数计算的误差通过梯度反向传播的方式对深度网络权值进行更新时，得到的梯度值接近0或特别大，也就是梯度消失或爆炸。梯度消失或梯度爆炸在本质原理上其实是一样的。<br>&emsp;&emsp;梯度爆炸将导致参数大小会出现很大的震荡，无法收敛；梯度消失将导致优化算法失效。</p>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>目前主流的梯度下降法是小批量梯度下降法（ps:可能部分的算法称之为sgd）,每次均利用部分的数据对函数进行优化，从而快速的达到收敛。<br><img src="/2020/08/28/sgd/10.png" alt><br>如图所示，该种算法每次迭代并不是最有效的，因此我们可以对其增设“动量”。<br>回顾一下梯度下降法每次的参数更新公式:</p>
<script type="math/tex; mode=display">
\begin{aligned}
W &:=W-\alpha \nabla W \\
b &:=b-\alpha \nabla b
\end{aligned}</script><p>可以看到，每次更新仅与当前梯度值相关, 并不涉及之前的梯度。而动量梯度下降法则对各个mini-batch求得的梯度 $\nabla W, \nabla b$ 使用指数加权平均得到 $V_{\nabla w}, V_{\nabla b_{\bullet}}$ 并使用新的参数更新之前的参数。<br>例如，在100次梯度下降中求得的梯度序列为:</p>
<script type="math/tex; mode=display">
\left\{\nabla W_{1}, \nabla W_{2}, \nabla W_{3} \ldots \ldots \ldots \nabla W_{99}, \nabla W_{100}\right\}</script><p>则其对应的动量梯度分别为：</p>
<script type="math/tex; mode=display">\begin{array}{c}
V_{\nabla W_{0}}=0 \\
V_{\nabla W_{1}}=\beta V_{\nabla W_{0}}+(1-\beta) \nabla W_{1} \\
V_{\nabla W_{2}}=\beta V_{\nabla W_{1}}+(1-\beta) \nabla W_{2} \\
·········· \\
V_{\nabla W_{100}}=\beta V_{\nabla W_{99}}+(1-\beta) \nabla W_{100}
\end{array}</script><p>使用指数加权平均之后梯度代替原梯度进行参数更新。因为每个指数加权平均后的梯度含有之前梯度的信息, 动量梯度下降法因此得名。其效果如下：<br><img src="/2020/08/28/sgd/11.png" alt><br>简单的来说，增设动量之后，将优化时的左右的摆动减小，让目标函数收敛的更快。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>入门</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>通俗理解奇异值分解</title>
    <url>/2020/08/27/svd-pca/</url>
    <content><![CDATA[<p>参考链接：</p>
<ol>
<li>[<a href="https://zhuanlan.zhihu.com/p/29846048">https://zhuanlan.zhihu.com/p/29846048</a>]</li>
<li>[<a href="https://www.zhihu.com/question/41120789/answer/481966094">https://www.zhihu.com/question/41120789/answer/481966094</a>]</li>
</ol>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>&emsp; &emsp;机器算法的核心就是如何妥善的处理数据，但是当我们接触到一大堆原始杂乱的陌生数据时，往往会感到手足无措，因此我们需要对原始数据压缩，从而找到影响结果变化的主要因素与次要因素，奇异值分解就为我们提供了相应的方法，便于我们从茫茫的数据中找到关键因素，当然这种方法不仅仅局限于数据压缩，还有其他强大的功能，在此就不一 一介绍了。</p>
<a id="more"></a>
<h2 id="特征值分解"><a href="#特征值分解" class="headerlink" title="特征值分解"></a>特征值分解</h2><p>&emsp; &emsp;在正式接触奇异值分解之前，我们先来回顾在线性代数中学到的特征值分解(对角化)。首先特征值分解具有一定的局限性，只能应用于方阵，并且该方阵还要满足每个特征值对应的特征向量线性无关的最大个数等于该特征值的重数才能够对角化。对于n维可对角化的矩阵A可写成以下形式：</p>
<script type="math/tex; mode=display">A=V \Lambda V^{-1}</script><p>具体表现形式为：</p>
<script type="math/tex; mode=display">A=\left[\begin{array}{}
\mid & \mid & \mid \\
\mathbf{v}_{1} & \mathbf{v}_{2} & \mathbf{v}_{3} \\
\mid & \left.\right|& \mid
\end{array}\right]\left[\begin{array}{}
\left.\mid \begin{array}{}
\lambda_{1} & 0 & 0 \\
0 & \lambda_{2} & 0 \\
0 & 0 & \lambda_{3}
\end{array}\right]\left[\left.\begin{array}{}
\mid & \mid & \mid \\
\mathbf{v}_{1} & \mathbf{v}_{2} & \mathbf{v}_{3}\\
\mid & \left.\right|& \mid
\end{array}\right]^{-1}\right.
\end{array}\right.</script><p>上式中，v1,v2,v3为矩阵A的特征向量，一般情况下我们会把特征向量标准化，即满足|| V ||=1。然而，这种特征值分解只能应用于部分方阵。然而，在我们实际的数据中，大部分都并非方阵，因此我们需要对这种方法进行拓展，从而提出了奇异值分解，来应对普遍情况。</p>
<h2 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h2><p>&emsp; &emsp;$ A A^{T} $与$ A^{T} A $这两个矩阵对于奇异值分解来说是十分重要的两种矩阵，它们具有以下特征：<br><br>&emsp; &emsp;&emsp; &emsp;1.对于任意维度的矩阵A，都存在$ A A^{T} $与$ A^{T} A $；<br><br>&emsp; &emsp;&emsp; &emsp;2.实对称矩阵，特征向量正交；<br>&emsp; &emsp;&emsp; &emsp;3.方阵；<br>&emsp; &emsp;&emsp; &emsp;4.半正定；<br>&emsp; &emsp;&emsp; &emsp;5.两者具有相同的正数特征值，该特征值得算术平方根称之为奇异值<br>&emsp; &emsp;&emsp; &emsp;6.两者的秩相等，且与矩阵A相同。<br>&emsp; &emsp;这样的话，我们令$ u_{i} $为矩阵$ A A^{T} $的特征向量，令$ v_{i} $为矩阵$ A^{T} A $的特征向量,则我们称由$ u_{i} $，$ v_{i} $构成的矩阵$U$，$V$为矩阵A的奇异向量，具体表达式如下：<br><img src="/2020/08/27/svd-pca/1.png" alt><br>也就是说：</p>
<script type="math/tex; mode=display">\left(A^{T} A\right) v_{i}=\lambda_{i} v_{i}</script><script type="math/tex; mode=display">\left(A A^{T}\right) u_{i}=\lambda_{i} u_{i}</script><p>&emsp; &emsp;根据之前$ A A^{T} $与$ A^{T} A $的性质可知，我们可以通过正交分解使得$U,V$为正交矩阵。有了上述的基础概念，下面引入核心公式，奇异值分解认为，任意维度的矩阵A都可以被分解为以下形式：</p>
<script type="math/tex; mode=display">A=U S V^{T}</script><p>其中，$U,V$就是矩阵A的奇异向量；S是对角阵，由奇异值构成。矩阵的排列顺序，一般按照奇异值从大到小排列，方便统一化管理和后续操作：<br><img src="/2020/08/27/svd-pca/2.png" alt><br>那么奇异值是如何计算的呢？</p>
<script type="math/tex; mode=display">A=U \Sigma V^{T} \Rightarrow A^{T}=V \Sigma U^{T} \Rightarrow A^{T} A=V \Sigma U^{T} U \Sigma V^{T}=V \Sigma^{2} V^{T}</script><p>所以说奇异值矩阵等于特征值矩阵的算术平方根。相比于之前的特征值分解，SVD可以应用于任意维度的矩阵，并且U,V是正交矩阵，S是对角阵，具体应用的时候会非常的方便。</p>
<h2 id="奇异值分解矩阵"><a href="#奇异值分解矩阵" class="headerlink" title="奇异值分解矩阵"></a>奇异值分解矩阵</h2><p>为了让大家有一个深刻的印象，我举一个简单的例子来具体说明SVD是如何分解矩阵的。<br>对于矩阵A：</p>
<script type="math/tex; mode=display">A=\left(\begin{array}{ccc}
3 & 2 & 2 \\
2 & 3 & -2
\end{array}\right)</script><p>计算$ A A^{T} $与$ A^{T} A $的值得：</p>
<script type="math/tex; mode=display">A A^{T}=\left(\begin{array}{cc}
17 & 8 \\
8 & 17
\end{array}\right), \quad A^{T} A=\left(\begin{array}{ccc}
13 & 12 & 2 \\
12 & 13 & -2 \\
2 & -2 & 8
\end{array}\right)</script><p>上述矩阵都是半正定的，其特征值为25、9，也就是说A的奇异值为5、2。另外，这两个矩阵之间的联系为：<br><img src="/2020/08/27/svd-pca/3.png" alt><br>因此SVD的表达式为：</p>
<script type="math/tex; mode=display">A=U S V^{T}=\left(\begin{array}{cc}
1 / \sqrt{2} & 1 / \sqrt{2} \\
1 / \sqrt{2} & -1 / \sqrt{2}
\end{array}\right)\left(\begin{array}{ccc}
5 & 0 & 0 \\
0 & 3 & 0
\end{array}\right)\left(\begin{array}{rrr}
1 / \sqrt{2} & 1 / \sqrt{2} & 0 \\
1 / \sqrt{18} & -1 / \sqrt{18} & 4 / \sqrt{18} \\
2 / 3 & -2 / 3 & -1 / 3
\end{array}\right)</script><p>上述所说的只是基本应用，下面我对SVD的公式进行基本的变换：</p>
<script type="math/tex; mode=display">\begin{array}{l}
A=U S V^{T} \\
A V=U S
\end{array}</script><p>也就是说SVD认为矩阵A是其所有奇异值与其对应的奇异向量的组合：</p>
<script type="math/tex; mode=display">A=\sigma_{1} u_{1} v_{1}^{T}+\ldots+\sigma_{r} u_{r} v_{r}^{T}</script><p>上述公式是理解SVD矩阵分解的关键，它提供了一种重要的方法去拆解m*n数据矩阵至r个元素，现在我们在来重新分析下之前的例子是如何工作的：</p>
<script type="math/tex; mode=display">A=\left(\begin{array}{ccc}
3 & 2 & 2 \\
2 & 3 & -2
\end{array}\right)</script><p>矩阵A可以被拆解为：</p>
<script type="math/tex; mode=display">\begin{aligned}
&=5\left(\begin{array}{ccc}
1 / \sqrt{2} \\
1 / \sqrt{2}
\end{array}\right)\left(\begin{array}{cccc}
1 / \sqrt{2} & 1 / \sqrt{2} & 0
\end{array}\right)+3\left(\begin{array}{cc}
1 / \sqrt{2} \\
-1 / \sqrt{2}
\end{array}\right)\left(\begin{array}{ccc}
1 / \sqrt{18} & -1 / \sqrt{18} & 4 / \sqrt{18}
\end{array}\right)\\
&=\left(\begin{array}{ccc}
3 & 2 & 2 \\
2 & 3 & -2
\end{array}\right)
\end{aligned}</script><h2 id="奇异值分解应用"><a href="#奇异值分解应用" class="headerlink" title="奇异值分解应用"></a>奇异值分解应用</h2><h3 id="moore-penrose伪逆"><a href="#moore-penrose伪逆" class="headerlink" title="moore-penrose伪逆"></a>moore-penrose伪逆</h3><p>对于线性方程组而言，我们往往需要计算矩阵的逆，如下图所示：</p>
<script type="math/tex; mode=display">\begin{aligned}
A x &=b \\
x &=A^{-1} b
\end{aligned}</script><p>但是对于大部分矩阵而言，都没有办法求其逆矩阵，因此在求解方程组时需要逆矩阵的替代品，moore-penrose伪逆提供了这样的一种方法，可以计算矩阵A的伪逆值，使得$ \left|A A^{+}-I_{n}\right|_{2} $ 最小，也就是伪逆矩阵$A^{+}$的相关性质最接近其逆矩阵，因此线性方程组的解就可以被估计为：</p>
<script type="math/tex; mode=display">\begin{aligned}
A x &=b \\
x & \approx A^{+} b
\end{aligned}</script><p>其中，为逆矩阵$A^{+}$的具体求解方式如下：</p>
<script type="math/tex; mode=display">\begin{array}{l}
A x=b \\
(U, D, V) \leftarrow \operatorname{svd}(A) \\
A^{+}=V D^{+} U^{T} \\
x=A^{+} b
\end{array}</script><p>我们以一个简单的例子来具体说明：<br>$A=\left[\begin{array}{ll}1 &amp; 1 \\ 1 &amp; 1\end{array}\right] \quad$ </p>
<p>$A=U \Sigma V^{T}=\frac{1}{\sqrt{2}}\left[\begin{array}{cc}1 &amp; 1 \\ 1 &amp; -1\end{array}\right]\left[\begin{array}{cc}2 &amp; 0 \\ 0 &amp; 0\end{array}\right] \frac{1}{\sqrt{2}}\left[\begin{array}{cc}1 &amp; 1 \\ 1 &amp; -1\end{array}\right]$</p>
<p>$A^{+}=V \Sigma^{+} U^{T}=\frac{1}{\sqrt{2}}\left[\begin{array}{cc}1 &amp; 1 \\ 1 &amp; -1\end{array}\right]\left[\begin{array}{cc}1 / 2 &amp; 0 \\ 0 &amp; 0\end{array}\right] \frac{1}{\sqrt{2}}\left[\begin{array}{cc}1 &amp; 1 \\ 1 &amp; -1\end{array}\right]=\frac{1}{4}\left[\begin{array}{cc}1 &amp; 1 \\ 1 &amp; 1\end{array}\right]$</p>
<p>&emsp;&emsp;moore-penrose伪逆的求解是基于奇异值分解的，通过上面的例子我们可以看出即便方阵不可逆，我们也能通过奇异值分解的方法得到其伪逆矩阵，也就是说该方法对于任意维度的任意矩阵都能适用。此外，待求矩阵A 的列数如果大于行数，得到就是所有可行解中L2范数最小的一个，如果相反，就是L2(Ax−y)最小</p>
<h3 id="数据压缩"><a href="#数据压缩" class="headerlink" title="数据压缩"></a>数据压缩</h3><p>&emsp;&emsp;在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵:</p>
<script type="math/tex; mode=display">A_{m \times n}=U_{m \times m} \Sigma_{m \times n} V_{n \times n}^{T} \approx U_{m \times k} \Sigma_{k \times k} V_{k \times n}^{T}</script><p>其中k要比n小很多，也就是一个大的矩阵A可以用三个小的矩阵$U_{m \times k}, \sum_{k \times k}, V_{k \times n}^{T}$来表示,这样就可以以舍弃部分精度来减少数据量。如下图所示，现在我们的矩阵A只需要灰色的部分的三个小矩阵就可以近似描述了。<br><img src="/2020/08/27/svd-pca/4.png" alt><br>由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪，PCA具体原理可以参考<a href="https://www.zhihu.com/question/41120789/answer/481966094"><strong>如何通俗易懂地讲解什么是 PCA 主成分分析</strong></a>。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引（LSI）。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>入门</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>基于SVD的图片分解</title>
    <url>/2020/08/28/svd-program/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇python基于numpy包实现SVD，因为有现成的API，所以直接调用即可。本代码实现对图片的奇异值分解效果展示</p>
<a id="more"></a>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">用户手册-by clever_bobo</span></span><br><span class="line"><span class="string">本函数用于测试奇异矩阵对图片进行分解，暂未设置任何纠错设置，请按照对应参数进行输入</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#读取文件，方块图像与风景图像</span></span><br><span class="line">im_square=mpimg.imread(<span class="string">r&quot;C:\Users\97751\Desktop\SVD\test1.png&quot;</span>)</span><br><span class="line">im_nature=mpimg.imread(<span class="string">r&quot;C:\Users\97751\Desktop\SVD\test2.png&quot;</span>)</span><br><span class="line"><span class="comment">#读取图片大小</span></span><br><span class="line">s1,s2,s3=im_square.shape</span><br><span class="line">n1,n2,n3=im_nature.shape</span><br><span class="line"><span class="comment">#转换格式</span></span><br><span class="line">im_square_temp = im_square.reshape(s1,s2 * s3)</span><br><span class="line">im_nature_temp = im_nature.reshape(n1, n2 * n3)</span><br><span class="line"><span class="comment">#调用numpy库中的线性函数的库</span></span><br><span class="line">U1,S1,VT1=np.linalg.svd(im_square_temp)</span><br><span class="line">U2,S2,VT2=np.linalg.svd(im_nature_temp)</span><br><span class="line"><span class="comment">#设置标志位</span></span><br><span class="line">flag=<span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> flag!=<span class="number">1</span>:</span><br><span class="line">    <span class="comment">#设置精度</span></span><br><span class="line">    nums=int(input(<span class="string">&quot;请输入保留的多少位奇异值：&quot;</span>))   </span><br><span class="line">    <span class="comment">#对方块图形进行SVD分解</span></span><br><span class="line">    img_square = (U1[:,<span class="number">0</span>:nums]).dot(np.diag(S1[<span class="number">0</span>:nums])).dot(VT1[<span class="number">0</span>:nums,:])</span><br><span class="line">    img_square= img_square.reshape(s1, s2 , s3)</span><br><span class="line">    <span class="comment">#对风景图形进行SVD分解</span></span><br><span class="line">    img_nature = (U2[:,<span class="number">0</span>:nums]).dot(np.diag(S2[<span class="number">0</span>:nums])).dot(VT2[<span class="number">0</span>:nums,:])</span><br><span class="line">    img_nature= img_nature.reshape(n1, n2 , n3)</span><br><span class="line">    <span class="comment">#精度分析</span></span><br><span class="line">    error1,error2=sum(S1[<span class="number">0</span>:nums])/sum(S1)*<span class="number">10000</span>//<span class="number">1</span>*<span class="number">0.01</span>,sum(S2[<span class="number">0</span>:nums])/sum(S2)*<span class="number">10000</span>//<span class="number">1</span>*<span class="number">0.01</span></span><br><span class="line">    <span class="comment">#画图部分</span></span><br><span class="line">    fig,ax=plt.subplots(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">    <span class="comment">#原始方块输出</span></span><br><span class="line">    ax[<span class="number">0</span>][<span class="number">0</span>].imshow(im_square)</span><br><span class="line">    ax[<span class="number">0</span>][<span class="number">0</span>].set(title = <span class="string">&quot;original square&quot;</span>)</span><br><span class="line">    <span class="comment">#SVD方块输出</span></span><br><span class="line">    ax[<span class="number">0</span>][<span class="number">1</span>].imshow(img_square)</span><br><span class="line">    ax[<span class="number">0</span>][<span class="number">1</span>].set(title = <span class="string">&quot; SVD&#x27;s square  precision=&quot;</span>+str(error1)+<span class="string">&#x27;%&#x27;</span>)</span><br><span class="line">    <span class="comment">#原始山脉输出</span></span><br><span class="line">    ax[<span class="number">1</span>][<span class="number">0</span>].imshow(im_nature)</span><br><span class="line">    ax[<span class="number">1</span>][<span class="number">0</span>].set(title = <span class="string">&quot;original nature&quot;</span>)</span><br><span class="line">    <span class="comment">#SVD山脉输出</span></span><br><span class="line">    ax[<span class="number">1</span>][<span class="number">1</span>].imshow(img_nature)</span><br><span class="line">    ax[<span class="number">1</span>][<span class="number">1</span>].set(title = <span class="string">&quot; SVD&#x27;s nature precision=&quot;</span>+str(error2)+<span class="string">&#x27;%&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    flag=int(input(<span class="string">&quot;是否结束SVD分解？是请输入1: &quot;</span>))</span><br><span class="line">print(<span class="string">&quot;感谢使用测试——by clever_bobo!!!&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="效果展示"><a href="#效果展示" class="headerlink" title="效果展示"></a>效果展示</h2><p>保留1位奇异值：<br><img src="/2020/08/28/svd-program/1.png" alt><br>保留5位奇异值：<br><img src="/2020/08/28/svd-program/2.png" alt><br>保留10位奇异值：<br><img src="/2020/08/28/svd-program/3.png" alt><br>保留50位奇异值：<br><img src="/2020/08/28/svd-program/4.png" alt><br>保留100位奇异值：<br><img src="/2020/08/28/svd-program/5.png" alt></p>
<p><strong>保留的奇异值位数越多，保留的信息就越多</strong></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>实例</tag>
      </tags>
  </entry>
</search>
